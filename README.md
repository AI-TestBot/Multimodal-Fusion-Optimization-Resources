# Artificial-Intelligence-Fusion-Resources

## Journal

*IEEE Transactions on Multimedia (TMM)*<br>

*IEEE Multimedia Magazine (MM)*<br>

*Information Fusion (IF)*<br>

## Coference

*ACM International Conference on Multimedia (ACM MM)*<br>

*IEEE International Conference on Multimedia & Expo (ICME)*<br>

## Group

**Multimodal Communication and Machine Learning Laboratory (MultiComp Lab)**<br>
*Louis-Philippe Morency*<br>
*Carnegie Mellon University*<br>
[[Homepage](http://multicomp.cs.cmu.edu/)]

## Resource

**awesome-multimodal-ml**<br>
[[Github](https://github.com/pliang279/awesome-multimodal-ml)]

**Awesome-Multimodal-Research**<br>
[[Github](https://github.com/Eurus-Holmes/Awesome-Multimodal-Research)]

## Review

**Multimodal Machine Learning A Survey and Taxonomy.**<br>
*Tadas Baltrušaitis; Chaitanya Ahuja; Louis-Philippe Morency.*<br>
TPAMI, 2019. [[PDF](https://arxiv.org/pdf/1705.09406)]

**Deep multimodal learning A survey on recent advances and trends.**<br>
*D Ramachandram, GW Taylor.*<br>
TIP, 2017. [[PDF](https://ieeexplore.ieee.org/abstract/document/8103116)]

**Multimodal intelligence: Representation learning, information fusion, and applications.**<br>
*C Zhang, Z Yang, X He, L Deng.*<br>
IEEE Journal of Selected Topics in Signal Processing, 2020. [[PDF](https://arxiv.org/pdf/1911.03977)]

**A survey on multimodal large language models.**<br>
*S Yin, C Fu, S Zhao, K Li, X Sun, T Xu, et al.*<br>
arXiv, 2023. [[PDF](https://arxiv.org/pdf/2306.13549)]

**Multimodal foundation models: From specialists to general-purpose assistants.**<br>
*C Li, Z Gan, Z Yang, J Yang, L Li, L Wang, J Gao.*<br>
Foundations and Trends® in Computer Graphics and Vision, 2024. [[PDF](https://arxiv.org/pdf/2306.13549)]

**Multimodality representation learning: A survey on evolution, pretraining and its applications.**<br>
*MA Manzoor, S Albarri, Z Xian, Z Meng, P Nakov, S Liang.*<br>
ACM Transactions on Multimedia Computing, Communications and Applications, 2023. [[PDF](https://arxiv.org/pdf/2302.00389)]

**Vlp: A survey on vision-language pre-training.**<br>
*FL Chen, DZ Zhang, ML Han, XY Chen, J Shi, S Xu, B Xu.*<br>
Machine Intelligence Research, 2023. [[PDF](https://link.springer.com/content/pdf/10.1007/s11633-022-1369-5.pdf)]

**Vision-language models for vision tasks: A survey.**<br>
*J Zhang, J Huang, S Jin, S Lu.*<br>
IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [[PDF](https://arxiv.org/pdf/2304.00685)]

**Mm-llms: Recent advances in multimodal large language models.**<br>
*D Zhang, Y Yu, C Li, J Dong, D Su, C Chu, D Yu.*<br>
arXiv:2401.13601, 2024. [[PDF](https://arxiv.org/pdf/2401.13601)]

## Representation

**DeViSE: A Deep Visual-Semantic Embedding Model.**<br>
*A Frome, G Corrado, J Shlens, S Bengio, J Dean, et al.*<br>
NIPS, 2013. [[PDF](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.466.176&rep=rep1&type=pdf)]

**Learning joint embedding with multimodal cues for cross-modal video-text retrieval.**<br>
*NC Mithun, J Li, F Metze, et al.*<br>
ICMR, 2018. [[PDF](https://dl.acm.org/doi/pdf/10.1145/3206025.3206064)]

**Deep multimodal representation learning: A survey.**<br>
*W Guo, J Wang, S Wang.*<br>
IEEE Access, 2019. [[PDF](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8715409)]

**Multibench: Multiscale benchmarks for multimodal representation learning.**<br>
*PP Liang, Y Lyu, X Fan, Z Wu, Y Cheng, J Wu, et al.*<br>
NeurlPS, 2021. 

## Translation

**Show and tell: A neural image caption generator.**<br>
*O Vinyals, A Toshev, S Bengio, et al.*<br>
CVPR, 2015. [[PDF](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf)]

**Show, attend and tell: Neural image caption generation with visual attention.**<br>
*K Xu, J Ba, R Kiros, K Cho, A Courville, et al.*<br>
PMLR, 2018. [[PDF](https://www.researchgate.net/publication/272194766_Show_Attend_and_Tell_Neural_Image_Caption_Generation_with_Visual_Attention)]

**A survey on automatic image caption generation.**<br>
*S Bai, S An.*<br>
Neurocomputing, 2018. [[PDF](http://press.liacs.nl/students.mir/inspiration/A%20survey%20on%20automatic%20image%20caption%20generation.Neurocomputing2018.pdf)]

## Alignment

**Stacked cross attention for image-text matching.**<br>
*KH Lee, X Chen, G Hua, H Hu, et al.*<br>
CVPR, 2018. [[PDF](https://openaccess.thecvf.com/content_ECCV_2018/papers/Kuang-Huei_Lee_Stacked_Cross_Attention_ECCV_2018_paper.pdf)]

**Visual Semantic Reasoning for Image-Text Matching.**<br>
*K Li, Y Zhang, K Li, Y Li, Y Fu.*<br>
CVPR, 2020. [[PDF](https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Visual_Semantic_Reasoning_for_Image-Text_Matching_ICCV_2019_paper.pdf)]

**Oscar Object-Semantics Aligned Pre-training for Vision-Language Tasks.**<br>
*X Li, X Yin, C Li, P Zhang, X Hu, L Zhang, et al.*<br>
ECCV, 2020. [[PDF](https://arxiv.org/pdf/2004.06165)]

**Fashionbert: Text and image matching with adaptive loss for cross-modal retrieval.**<br>
*D Gao, L Jin, B Chen, M Qiu, P Li, Y Wei, Y Hu, et al.*<br>
SIGIR, 2020. [[PDF](https://dl.acm.org/doi/pdf/10.1145/3397271.3401430)]

**Similarity Reasoning and Filtration for Image-Text Matching.**<br>
*H Diao, Y Zhang, L Ma, H Lu.*<br>
AAAI, 2021. [[PDF](https://arxiv.org/pdf/2101.01368.pdf)]

## Fusion

**Attention-Based Multimodal Fusion for Video Description.**<br>
*C Hori, T Hori, TY Lee, Z Zhang, et al.*<br>
CVPR, 2017. [[PDF](https://openaccess.thecvf.com/content_ICCV_2017/papers/Hori_Attention-Based_Multimodal_Fusion_ICCV_2017_paper.pdf)]

**Multimodal keyless attention fusion for video classification.**<br>
*X Long, C Gan, G De Melo, X Liu, Y Li, F Li, et al.*<br>
AAAI, 2018. [[PDF](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/17054/16313)]

**MFAS Multimodal Fusion Architecture Search.**<br>
*JM Pérez-Rúa, V Vielzeuf, S Pateux, et al.*<br>
CVPR, 2019. [[PDF](https://openaccess.thecvf.com/content_CVPR_2019/papers/Perez-Rua_MFAS_Multimodal_Fusion_Architecture_Search_CVPR_2019_paper.pdf)]

**Quantum-inspired multimodal fusion for video sentiment analysis.**<br>
*Q Li, D Gkoumas, C Lioma, M Melucci.*<br>
Information Fusion, 2020. [[PDF](https://arxiv.org/pdf/2103.10572.pdf)]

**A survey on machine learning for data fusion.**<br>
*T Meng, X Jing, Z Yan, W Pedrycz.*<br>
Information Fusion, 2020. [[PDF](https://www.researchgate.net/publication/337865553_A_Survey_on_Machine_Learning_for_Data_Fusion)]

**Attention bottlenecks for multimodal fusion.**<br>
*A Nagrani, S Yang, A Arnab, A Jansen, et al.*<br>
NIPS, 2021. [[PDF](https://proceedings.neurips.cc/paper/2021/file/76ba9f564ebbc35b1014ac498fafadd0-Paper.pdf)]

**Dynamic Multimodal Fusion.**<br>
*Z Xue, R Marculescu.*<br>
ArXiv, 2022. [[PDF](https://arxiv.org/pdf/2204.00102.pdf)]

## Co-learning

**Visualbert: A simple and performant baseline for vision and language.**<br>
*LH Li, M Yatskar, D Yin, CJ Hsieh, et al.*<br>
ArXiv, 2019. 

**Lxmert: Learning cross-modality encoder representations from transformers.**<br>
*H Tan, M Bansal.*<br>
EMNLP, 2019. [[PDF](https://arxiv.org/pdf/1908.07490.pdf)]

**Videobert: A joint model for video and language representation learning.**<br>
*C Sun, A Myers, C Vondrick, et al.*<br>
ICCV, 2019. 

**Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks.**<br>
*J Lu, D Batra, D Parikh, S Lee.*<br>
NIPS, 2019. [[PDF](https://arxiv.org/pdf/1908.02265.pdf%20http://arxiv.org/abs/1908.02265.pdf)]

**Univl: A unified video and language pre-training model for multimodal understanding and generation.**<br>
*H Luo, L Ji, B Shi, H Huang, N Duan, T Li, J Li, et al.*<br>
ArXiv, 2020. [[PDF](https://arxiv.org/pdf/2002.06353.pdf)]

**Actbert: Learning global-local video-text representations.**<br>
*L Zhu, Y Yang.*<br>
CVPR, 2020. 

**What Makes Training Multi-modal Classification Networks Hard.**<br>
*W Wang, D Tran, M Feiszli.*<br>
CVPR, 2020. [[PDF](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_What_Makes_Training_Multi-Modal_Classification_Networks_Hard_CVPR_2020_paper.pdf)]

**Hero: Hierarchical encoder for video+ language omni-representation pre-training.**<br>
*L Li, YC Chen, Y Cheng, Z Gan, L Yu, J Liu.*<br>
EMNLP, 2020. 

**Vl-bert: Pre-training of generic visual-linguistic representations.**<br>
*W Su, X Zhu, Y Cao, B Li, L Lu, F Wei, J Dai.*<br>
ICLR, 2020. 

**Foundations of multimodal co-learning.**<br>
*A Zadeh, PP Liang, LP Morency.*<br>
Information Fusion, 2020. [[PDF](https://www.sciencedirect.com/science/article/pii/S1566253520303006)]

**Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning.**<br>
*W Li, C Gao, G Niu, X Xiao, H Liu, J Liu, H Wu, et al.*<br>
ArXiv, 2021. [[PDF](https://arxiv.org/pdf/2012.15409.pdf)]

**M6: A chinese multimodal pretrainer.**<br>
*J Lin, R Men, A Yang, C Zhou, M Ding, Y Zhang, et al.*<br>
ArXiv, 2021.

**Multimodal Co-learning: Challenges, Applications with Datasets, Recent Advances and Future Directions.**<br>
*A Rahate, R Walambe, S Ramanna, et al.*<br>
ArXiv, 2021. [[PDF](https://arxiv.org/ftp/arxiv/papers/2107/2107.13782.pdf)]

**Towards a Unified Foundation Model: Jointly Pre-Training Transformers on Unpaired Images and Text.**<br>
*Q Li, B Gong, Y Cui, D Kondratyuk, X Du, et al.*<br>
ArXiv, 2021.

**VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts.**<br>
*W Wang, H Bao, L Dong, F Wei.*<br>
ArXiv, 2021.

**WenLan: Bridging vision and language by large-scale multi-modal pre-training.**<br>
*Y Huo, M Zhang, G Liu, H Lu, Y Gao, G Yang, et al.*<br>
ArXiv, 2021.

**WenLan 2.0: Make AI Imagine via a Multimodal Foundation Model.**<br>
*N Fei, Z Lu, Y Gao, G Yang, Y Huo, J Wen, H Lu, R Song, X Gao, T Xiang, et al.*<br>
ArXiv, 2021.

**Learning transferable visual models from natural language supervision.**<br>
*A Radford, JW Kim, C Hallacy, et al.*<br>
ICML, 2021.

**Vilt: Vision-and-language transformer without convolution or region supervision.**<br>
*W Kim, B Son, I Kim.*<br>
ICML, 2021.

**Zero-shot text-to-image generation.**<br>
*A Ramesh, M Pavlov, G Goh, S Gray, et al.*<br>
ICML, 2021.

**Multimodal few-shot learning with frozen language models.**<br>
*M Tsimpoukelli, J Menick, S Cabi, et al.*<br>
NIPS, 2021.

**Wukong: 100 Million Large-scale Chinese Cross-modal Pre-training Dataset and A Foundation Framework.**<br>
*J Gu, X Meng, G Lu, L Hou, M Niu, H Xu, et al.*<br>
ArXiv, 2022.

## Applications

### Understanding

#### Image

**Deep Collaborative Embedding for Social Image Understanding.**<br>
*Z Li, J Tang, T Mei.*<br>
TPAMI, 2019. 
[[PDF](https://ieeexplore.ieee.org/document/8403294)]

**Layoutlm: Pre-training of text and layout for document image understanding.**<br>
*Y Xu, M Li, L Cui, S Huang, F Wei, M Zhou.*<br>
KDD, 2020. 

**Beyond visual semantics: Exploring the role of scene text in image understanding.**<br>
*AU Dey, SK Ghosh, E Valveny, G Harit.*<br>
Pattern Recognition Letters, 2021. 

#### Video 

**Eco Efficient convolutional network for online video understanding.**<br>
*M Zolfaghari, K Singh, T Brox.*<br>
ECCV, 2018. 
[[PDF](https://openaccess.thecvf.com/content_ECCV_2018/papers/Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper.pdf)]

**TSM Temporal Shift Module for Efficient Video Understanding.**<br>
*J Lin, C Gan, S Han.*<br>
CVPR, 2019. 
[[PDF](https://openaccess.thecvf.com/content_ICCV_2019/papers/Lin_TSM_Temporal_Shift_Module_for_Efficient_Video_Understanding_ICCV_2019_paper.pdf)]

**Assemblenet: Searching for multi-stream neural connectivity in video architectures.**<br>
*MS Ryoo, AJ Piergiovanni, M Tan, et al.*<br>
ICLR, 2020. 

**Is Space-Time Attention All You Need for Video Understanding?**<br>
*G Bertasius, H Wang, et al.*<br>
ArXiv, 2021. 

**MDMMT Multidomain Multimodal Transformer for Video Retrieval.**<br>
*M Dzabraev, M Kalashnikov, et al.*<br>
CVPR, 2021. 
[[PDF](https://openaccess.thecvf.com/content/CVPR2021W/HVU/papers/Dzabraev_MDMMT_Multidomain_Multimodal_Transformer_for_Video_Retrieval_CVPRW_2021_paper.pdf)]

# Artificial-Intelligence-Alignment-Resources

## Review

**Large language model alignment: A survey.**<br>
*T Shen, R Jin, Y Huang, C Liu, W Dong, Z Guo, X Wu, Y Liu, D Xiong.*<br>
arXiv:2309.15025, 2023.
[[ArXiv](https://arxiv.org/pdf/2309.15025)]

**Ai alignment: A comprehensive survey.**<br>
*J Ji, T Qiu, B Chen, B Zhang, H Lou, K Wang, Y Duan, Z He, J Zhou, Z Zhang, F Zeng, KY Ng, et al.*<br>
arXiv:2310.19852, 2023.
[[ArXiv](https://arxiv.org/pdf/2310.19852)]
[[Homepage](https://alignmentsurvey.com/)]

**Aligning large language models with human: A survey.**<br>
*Y Wang, W Zhong, L Li, F Mi, X Zeng, W Huang, L Shang, X Jiang, Q Liu.*<br>
arXiv:2307.12966, 2023.
[[ArXiv](https://arxiv.org/pdf/2307.12966)]
[[Homepage](https://github.com/GaryYufei/AlignLLMHumanSurvey/)]

## Capacity Alignment

**Making large language models better reasoners with alignment.**<br>
*P Wang, L Li, L Chen, F Song, B Lin, Y Cao, T Liu, Z Sui.*<br>
arXiv:2309.02144, 2023.
[[Paper](https://arxiv.org/pdf/2309.02144)]

## Preference Alignment

**Preference ranking optimization for human alignment.**<br>
*F Song, B Yu, M Li, H Yu, F Huang, Y Li, et al.*<br>
AAAI, 2024.
[[Paper](https://ojs.aaai.org/index.php/AAAI/article/download/29865/31509)]

**Aligner: Achieving efficient alignment through weak-to-strong correction.**<br>
*J Ji, B Chen, H Lou, D Hong, B Zhang, X Pan, et al.*<br>
arXiv, 2024.
[[ArXiv](https://arxiv.org/pdf/2402.02416)]
[[HomePage](https://aligner2024.github.io/)]

**Knowledgeable preference alignment for llms in domain-specific question answering.**<br>
*Y Zhang, Z Chen, Y Fang, L Cheng, Y Lu, F Li, W Zhang, H Chen.*<br>
arXiv:2311.06503, 2023.
[[ArXiv](https://arxiv.org/pdf/2311.06503)]
[[Github](https://github.com/zjukg/KnowPAT)]

## Value Alignment

**Aligning ai with shared human values.**<br>
*D Hendrycks, C Burns, S Basart, A Critch, J Li, D Song, J Steinhardt.*<br>
ICLR, 2021.
[[ArXiv](https://arxiv.org/pdf/2008.02275)]

**Safe rlhf: Safe reinforcement learning from human feedback.**<br>
*J Dai, X Pan, R Sun, J Ji, X Xu, M Liu, Y Wang, et al.*<br>
arXiv, 2023.
[[ArXiv](https://arxiv.org/pdf/2310.12773)]
[[Github](https://github.com/PKU-Alignment/safe-rlhf)]

**A Moral Imperative: The Need for Continual Superalignment of Large Language Models.**<br>
*G Puthumanaillam, M Vora, P Thangeda, M Ornik.*<br>
arXiv:2403.14683, 2024.
[[ArXiv](https://arxiv.org/pdf/2403.14683)]

## Security Aligenment

**A survey of safety and trustworthiness of large language models through the lens of verification and validation.**<br>
*X Huang, W Ruan, W Huang, G **, Y Dong, C Wu, S Bensalem, R Mu, Y Qi, X Zhao, K Cai, et al.*<br>
arxiv:2305.11391, 2023.
[[ArXiv](https://arxiv.org/pdf/2305.11391)]

**A survey on large language model (llm) security and privacy: The good, the bad, and the ugly.**<br>
*Y Yao, J Duan, K Xu, Y Cai, Z Sun, Y Zhang.*<br>
High-Confidence Computing, 2024.
[[Paper](https://www.sciencedirect.com/science/article/pii/S266729522400014X)]

**Safeguarding Large Language Models: A Survey.**<br>
*Y Dong, R Mu, Y Zhang, S Sun, T Zhang, C Wu, G Jin, Y Qi, J Hu, J Meng, S Bensalem, et al.*<br>
arXiv:2406.02622, 2024.
[[ArXiv](https://arxiv.org/pdf/2406.02622)]

## Methods

### PEFT

**Parameter-efficient fine-tuning of large-scale pre-trained language models.**<br>
*N Ding, Y Qin, G Yang, F Wei, Z Yang, Y Su, S Hu, Y Chen, CM Chan, W Chen, J Yi, W Zhao, et al.*<br>
Nature Machine Intelligence, 2023.
[[Paper](https://www.nature.com/articles/s42256-023-00626-4)]

**Lora: Low-rank adaptation of large language models.**<br>
*EJ Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen.*<br>
arXiv:2106.09685, 2021.
[[Paper](https://arxiv.org/pdf/2106.09685))]

**Longlora: Efficient fine-tuning of long-context large language models.**<br>
*Y Chen, S Qian, H Tang, X Lai, Z Liu, S Han, J Jia.*<br>
arXiv:2309.12307, 2023.
[[ArXiv](https://arxiv.org/pdf/2309.12307)]
[[Github](https://github.com/dvlab-research/LongLoRA)]

## DataSets

**Openassistant conversations-democratizing large language model alignment.**<br>
*A Köpf, Y Kilcher, D von Rütte, S Anagnostidis, ZR Tam, K Stevens, A Barhoum, D Nguyen, et al.*<br>
Advances in Neural Information Processing Systems, 2024.
[[Paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/949f0f8f32267d297c2d4e3ee10a2e7e-Paper-Datasets_and_Benchmarks.pdf)]
[[Homepage](https://www.ykilcher.com/oa-contributors)]

## Tools

**NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment.**<br>
*G Shen, Z Wang, O Delalleau, J Zeng, Y Dong, D Egert, S Sun, J Zhang, S Jain, et al.*<br>
arXiv:2405.01481, 2024.
[[ArXiv](https://arxiv.org/pdf/2405.01481)]
[[Github](https://github.com/NVIDIA/NeMo-Aligner)]

